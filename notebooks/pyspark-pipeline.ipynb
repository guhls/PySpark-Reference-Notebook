{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da2a0bc9",
   "metadata": {},
   "source": [
    "- [Pyspark-With-Python](https://github.com/krishnaik06/Pyspark-With-Python)\n",
    "- [Pyspark Docs](https://spark.apache.org/docs/latest/api/python/getting_started/install.html#python-version-supported)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2cd45c",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639563a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5be4ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec5eeb05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endTime</th>\n",
       "      <th>artistName</th>\n",
       "      <th>trackName</th>\n",
       "      <th>msPlayed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-11-09 13:20</td>\n",
       "      <td>24kGoldn</td>\n",
       "      <td>Mood (feat. iann dior)</td>\n",
       "      <td>61970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-11-11 19:02</td>\n",
       "      <td>Drake</td>\n",
       "      <td>Fair Trade (with Travis Scott)</td>\n",
       "      <td>270006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-11-16 11:17</td>\n",
       "      <td>Liga/Desliga</td>\n",
       "      <td>PEGADINHAS | Liga - Terça, 16 de Novembro</td>\n",
       "      <td>161132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-11-16 11:18</td>\n",
       "      <td>Kygo</td>\n",
       "      <td>Undeniable (feat. X Ambassadors)</td>\n",
       "      <td>6360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-11-16 11:21</td>\n",
       "      <td>Masego</td>\n",
       "      <td>Garden Party</td>\n",
       "      <td>17293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            endTime    artistName                                  trackName  \\\n",
       "0  2021-11-09 13:20      24kGoldn                     Mood (feat. iann dior)   \n",
       "1  2021-11-11 19:02         Drake             Fair Trade (with Travis Scott)   \n",
       "2  2021-11-16 11:17  Liga/Desliga  PEGADINHAS | Liga - Terça, 16 de Novembro   \n",
       "3  2021-11-16 11:18          Kygo           Undeniable (feat. X Ambassadors)   \n",
       "4  2021-11-16 11:21        Masego                               Garden Party   \n",
       "\n",
       "   msPlayed  \n",
       "0     61970  \n",
       "1    270006  \n",
       "2    161132  \n",
       "3      6360  \n",
       "4     17293  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('../asset/spotify_history.csv').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c5b2fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark Session\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05895321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/15 19:02:21 WARN Utils: Your hostname, not resolves to a loopback address: 127.0.1.1; using 192.168.15.156 instead (on interface wlp1s0)\n",
      "22/11/15 19:02:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/15 19:02:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('FirstSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88bb4405",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.156:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FirstSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f547c33b250>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db958bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------------+--------------------+--------+\n",
      "|         endTime|  artistName|           trackName|msPlayed|\n",
      "+----------------+------------+--------------------+--------+\n",
      "|2021-11-09 13:20|    24kGoldn|Mood (feat. iann ...|   61970|\n",
      "|2021-11-11 19:02|       Drake|Fair Trade (with ...|  270006|\n",
      "|2021-11-16 11:17|Liga/Desliga|PEGADINHAS | Liga...|  161132|\n",
      "|2021-11-16 11:18|        Kygo|Undeniable (feat....|    6360|\n",
      "|2021-11-16 11:21|      Masego|        Garden Party|   17293|\n",
      "+----------------+------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('../asset/spotify_history.csv', header=True) # %timeit: 359 ms\n",
    "#df_pyspark = spark.read.option('header', 'true').csv('../asset/spotify_history.csv') # %timeit: 432 ms\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b913e7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- endTime: string (nullable = true)\n",
      " |-- artistName: string (nullable = true)\n",
      " |-- trackName: string (nullable = true)\n",
      " |-- msPlayed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7271e9a5",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "\n",
    "- PySpark Dataframe\n",
    "- Reading The Dataset\n",
    "- Checking the Datatypes of the Column(Schema)\n",
    "- Selecting Columns And Indexing\n",
    "- Check Describe option similar to Pandas\n",
    "- Adding Columns\n",
    "- Dropping columns\n",
    "- Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a2d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc5721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SecondSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33e04e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.15.156:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SecondSession</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f54d344efe0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a4a3610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+--------+\n",
      "|            endTime|  artistName|           trackName|msPlayed|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "|2021-11-09 13:20:00|    24kGoldn|Mood (feat. iann ...|   61970|\n",
      "|2021-11-11 19:02:00|       Drake|Fair Trade (with ...|  270006|\n",
      "|2021-11-16 11:17:00|Liga/Desliga|PEGADINHAS | Liga...|  161132|\n",
      "|2021-11-16 11:18:00|        Kygo|Undeniable (feat....|    6360|\n",
      "|2021-11-16 11:21:00|      Masego|        Garden Party|   17293|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n",
      "root\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- artistName: string (nullable = true)\n",
      " |-- trackName: string (nullable = true)\n",
      " |-- msPlayed: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "df_pyspark = spark.read.option('header', 'true').csv('../asset/spotify_history.csv', inferSchema=True)\n",
    "print(df_pyspark.show(5))\n",
    "print(df_pyspark.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bef518b",
   "metadata": {},
   "source": [
    " **Data Frames** is data structures, that can peform various kinds of operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e41d319f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['endTime', 'artistName', 'trackName', 'msPlayed']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get Columns\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d52d69e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(endTime=datetime.datetime(2021, 11, 9, 13, 20), artistName='24kGoldn', trackName='Mood (feat. iann dior)', msPlayed=61970),\n",
       " Row(endTime=datetime.datetime(2021, 11, 11, 19, 2), artistName='Drake', trackName='Fair Trade (with Travis Scott)', msPlayed=270006),\n",
       " Row(endTime=datetime.datetime(2021, 11, 16, 11, 17), artistName='Liga/Desliga', trackName='PEGADINHAS | Liga - Terça, 16 de Novembro', msPlayed=161132)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e0daa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|  artistName|\n",
      "+------------+\n",
      "|    24kGoldn|\n",
      "|       Drake|\n",
      "|Liga/Desliga|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select column\n",
    "df_pyspark.select('artistName').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1c357610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------------------+\n",
      "|  artistName|           trackName|\n",
      "+------------+--------------------+\n",
      "|    24kGoldn|Mood (feat. iann ...|\n",
      "|       Drake|Fair Trade (with ...|\n",
      "|Liga/Desliga|PEGADINHAS | Liga...|\n",
      "+------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['artistName', 'trackName']).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f000bf56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('endTime', 'timestamp'),\n",
       " ('artistName', 'string'),\n",
       " ('trackName', 'string'),\n",
       " ('msPlayed', 'int')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f2e23e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+--------------------+------------------+\n",
      "|summary|artistName|           trackName|          msPlayed|\n",
      "+-------+----------+--------------------+------------------+\n",
      "|  count|      9044|                9044|              9044|\n",
      "|   mean|    3030.0|            Infinity|109133.91287041132|\n",
      "| stddev|      null|                 NaN|102590.48463600103|\n",
      "|    min|      $NOT|\"Surface Pressure...|                 0|\n",
      "|    max| אריאל לוי|相信一切是最好的安排|           1564330|\n",
      "+-------+----------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "436fce7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- artistName: string (nullable = true)\n",
      " |-- trackName: string (nullable = true)\n",
      " |-- msPlayed: integer (nullable = true)\n",
      " |-- secondsPlayed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adding columns\n",
    "df_pyspark = df_pyspark.withColumn('secondsPlayed', df_pyspark['msPlayed']/60_000)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d92655e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+--------+------------------+\n",
      "|            endTime|  artistName|           trackName|msPlayed|     secondsPlayed|\n",
      "+-------------------+------------+--------------------+--------+------------------+\n",
      "|2021-11-09 13:20:00|    24kGoldn|Mood (feat. iann ...|   61970|1.0328333333333333|\n",
      "|2021-11-11 19:02:00|       Drake|Fair Trade (with ...|  270006|            4.5001|\n",
      "|2021-11-16 11:17:00|Liga/Desliga|PEGADINHAS | Liga...|  161132|2.6855333333333333|\n",
      "|2021-11-16 11:18:00|        Kygo|Undeniable (feat....|    6360|             0.106|\n",
      "|2021-11-16 11:21:00|      Masego|        Garden Party|   17293|0.2882166666666667|\n",
      "+-------------------+------------+--------------------+--------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34ff8d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+------------------+\n",
      "|            endTime|  artistName|           trackName|     secondsPlayed|\n",
      "+-------------------+------------+--------------------+------------------+\n",
      "|2021-11-09 13:20:00|    24kGoldn|Mood (feat. iann ...|1.0328333333333333|\n",
      "|2021-11-11 19:02:00|       Drake|Fair Trade (with ...|            4.5001|\n",
      "|2021-11-16 11:17:00|Liga/Desliga|PEGADINHAS | Liga...|2.6855333333333333|\n",
      "+-------------------+------------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.drop('msPlayed')\n",
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4898f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+------------------+\n",
      "|            endTime|  artistName|           trackName|     minutesPlayed|\n",
      "+-------------------+------------+--------------------+------------------+\n",
      "|2021-11-09 13:20:00|    24kGoldn|Mood (feat. iann ...|1.0328333333333333|\n",
      "|2021-11-11 19:02:00|       Drake|Fair Trade (with ...|            4.5001|\n",
      "|2021-11-16 11:17:00|Liga/Desliga|PEGADINHAS | Liga...|2.6855333333333333|\n",
      "+-------------------+------------+--------------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumnRenamed('secondsPlayed', 'minutesPlayed')\n",
    "df_pyspark.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff637b58",
   "metadata": {},
   "source": [
    "# Part 3\n",
    "- Dropping Columns\n",
    "- Dropping Rows\n",
    "- Various Parameter In Dropping functionalities\n",
    "- Handling Missing values by Mean, MEdian And Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "606d0c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('ThirdSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fddf671e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+--------------------+--------+\n",
      "|            endTime|artistName|           trackName|msPlayed|\n",
      "+-------------------+----------+--------------------+--------+\n",
      "|2021-11-09 13:20:00|  24kGoldn|Mood (feat. iann ...|   61970|\n",
      "|2021-11-11 19:02:00|     Drake|Fair Trade (with ...|  270006|\n",
      "+-------------------+----------+--------------------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('../asset/spotify_history.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87e54852",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[endTime: timestamp, artistName: string, trackName: string, msPlayed: int]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop na rows\n",
    "df_pyspark.na.drop() # df_pyspark.na.drop(how='any')\n",
    "\n",
    "# Drop only rows if all columns values are null\n",
    "df_pyspark.na.drop(how='all')\n",
    "\n",
    "# Drop only rows if an specific amount of null in columns values\n",
    "df_pyspark.na.drop(how='any', thresh=2)\n",
    "\n",
    "# Drop only rows in specifics columns are null\n",
    "df_pyspark.na.drop(how='any', subset=['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd006d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na rows\n",
    "df_pyspark.na.fill('Missign Values', ['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0cc6c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employees_pyspark = spark.read.csv('../Pyspark-With-Python-main/test2.csv', header=True, inferSchema=True)\n",
    "df_employees_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1bbf009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill na with ml module \n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "cols = ['age', 'Experience', 'Salary']\n",
    "\n",
    "# Imputer will get mean/median in coluns and place in null values\n",
    "imputer = Imputer(\n",
    "    inputCols=cols,\n",
    "    outputCols=[f'{col}_imputed' for col in cols]\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1039b748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|     Name| age|Experience|Salary|age_imputed|Experience_imputed|Salary_imputed|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "|    Krish|  31|        10| 30000|         31|                10|         30000|\n",
      "|Sudhanshu|  30|         8| 25000|         30|                 8|         25000|\n",
      "|    Sunny|  29|         4| 20000|         29|                 4|         20000|\n",
      "|     Paul|  24|         3| 20000|         24|                 3|         20000|\n",
      "|   Harsha|  21|         1| 15000|         21|                 1|         15000|\n",
      "|  Shubham|  23|         2| 18000|         23|                 2|         18000|\n",
      "|   Mahesh|null|      null| 40000|         28|                 5|         40000|\n",
      "|     null|  34|        10| 38000|         34|                10|         38000|\n",
      "|     null|  36|      null|  null|         36|                 5|         25750|\n",
      "+---------+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Add imputation cols to df\n",
    "imputer.fit(df_employees_pyspark).transform(df_employees_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a4c7d5",
   "metadata": {},
   "source": [
    "# Part 4\n",
    "- Filter Operation\n",
    "- &,|,==\n",
    "- ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe86eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('FourthSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "428cc7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- endTime: timestamp (nullable = true)\n",
      " |-- artistName: string (nullable = true)\n",
      " |-- trackName: string (nullable = true)\n",
      " |-- msPlayed: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+-------------------+------------+--------------------+--------+\n",
      "|            endTime|  artistName|           trackName|msPlayed|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "|2021-11-09 13:20:00|    24kGoldn|Mood (feat. iann ...|   61970|\n",
      "|2021-11-11 19:02:00|       Drake|Fair Trade (with ...|  270006|\n",
      "|2021-11-16 11:17:00|Liga/Desliga|PEGADINHAS | Liga...|  161132|\n",
      "|2021-11-16 11:18:00|        Kygo|Undeniable (feat....|    6360|\n",
      "|2021-11-16 11:21:00|      Masego|        Garden Party|   17293|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('../asset/spotify_history.csv', header=True, inferSchema=True)\n",
    "print(df_pyspark.printSchema())\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3067210d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+--------+\n",
      "|            endTime|          artistName|           trackName|msPlayed|\n",
      "+-------------------+--------------------+--------------------+--------+\n",
      "|2021-11-11 19:02:00|               Drake|Fair Trade (with ...|  270006|\n",
      "|2021-11-16 11:34:00|           O Assunto|Inflação: a do Br...|  500371|\n",
      "|2021-11-16 11:53:00|Matando Robôs Gig...|MRG 579: Arcane -...| 1143985|\n",
      "|2021-11-16 12:09:00|           Lil Nas X|INDUSTRY BABY (fe...|  211999|\n",
      "|2021-11-16 13:28:00|           YNW Melly|           Bang Bang|  260333|\n",
      "+-------------------+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('msPlayed >= 200000').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21631a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------+\n",
      "|          artistName|           trackName|msPlayed|\n",
      "+--------------------+--------------------+--------+\n",
      "|Matando Robôs Gig...|MRG 579: Arcane -...| 1143985|\n",
      "|Respondendo em Vo...|T2E17 — Por que s...| 1050221|\n",
      "|The Joe Rogan Exp...|JRE MMA Show #130...| 1510353|\n",
      "|The Joe Rogan Exp...|#1876 - Greg Fitz...| 1564330|\n",
      "|Data Science Academy|Episódio 46 - Mig...| 1348179|\n",
      "+--------------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('msPlayed >= 1000000').select(['artistName', 'trackName', 'msPlayed']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c0718f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------------+--------+\n",
      "|            endTime|   artistName|           trackName|msPlayed|\n",
      "+-------------------+-------------+--------------------+--------+\n",
      "|2021-11-16 12:26:00|   Juice WRLD|Come & Go (with M...|   85259|\n",
      "|2021-11-18 12:36:00|Clint Mansell|         Lux Aeterna|   90475|\n",
      "|2021-11-22 11:26:00|     24kGoldn|Mood (feat. iann ...|   81833|\n",
      "+-------------------+-------------+--------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------------------+--------------------+--------------------+--------+\n",
      "|            endTime|          artistName|           trackName|msPlayed|\n",
      "+-------------------+--------------------+--------------------+--------+\n",
      "|2022-10-08 03:49:00|Data Science Academy|Episódio 46 - Mig...| 1348179|\n",
      "+-------------------+--------------------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter((df_pyspark['msPlayed']>80000) & \n",
    "                  (df_pyspark['msPlayed']<100000)).show(3)\n",
    "\n",
    "df_pyspark.filter((df_pyspark['msPlayed']==20000) | \n",
    "                  (df_pyspark['artistName']=='Data Science Academy')).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59bd5f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+--------+\n",
      "|            endTime|  artistName|           trackName|msPlayed|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "|2021-11-11 19:02:00|       Drake|Fair Trade (with ...|  270006|\n",
      "|2021-11-16 11:17:00|Liga/Desliga|PEGADINHAS | Liga...|  161132|\n",
      "|2021-11-16 11:18:00|        Kygo|Undeniable (feat....|    6360|\n",
      "|2021-11-16 11:21:00|      Masego|        Garden Party|   17293|\n",
      "|2021-11-16 11:21:00|     Fleurie|Breathe - Legends...|  184163|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Not operator\n",
    "df_pyspark.filter(~(df_pyspark['artistName'] == '24kGoldn')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079a85a9",
   "metadata": {},
   "source": [
    "# Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0303a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark =  SparkSession.builder.appName('FifthSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57ba701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+--------------------+--------+\n",
      "|            endTime|  artistName|           trackName|msPlayed|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "|2021-11-09 13:20:00|    24kGoldn|Mood (feat. iann ...|   61970|\n",
      "|2021-11-11 19:02:00|       Drake|Fair Trade (with ...|  270006|\n",
      "|2021-11-16 11:17:00|Liga/Desliga|PEGADINHAS | Liga...|  161132|\n",
      "|2021-11-16 11:18:00|        Kygo|Undeniable (feat....|    6360|\n",
      "|2021-11-16 11:21:00|      Masego|        Garden Party|   17293|\n",
      "+-------------------+------------+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('../asset/spotify_history.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41ad96b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+\n",
      "|artistName|sum(msPlayed)|\n",
      "+----------+-------------+\n",
      "| Lil Nas X|     35301200|\n",
      "|THE SCOTTS|       706556|\n",
      "|Snoop Dogg|      2110118|\n",
      "+----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group by\n",
    "\n",
    "# Its needded apply a group by functionality first and then a apply aggregate function(ex:. sum, max, mean, ...)\n",
    "df_pyspark.groupBy('artistName').sum().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d275e5cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|artistName|count|\n",
      "+----------+-----+\n",
      "| Lil Nas X|  283|\n",
      "|THE SCOTTS|   13|\n",
      "|Snoop Dogg|   18|\n",
      "+----------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('artistName').count().show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb7fa519",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(msPlayed)|\n",
      "+-------------+\n",
      "|    987007108|\n",
      "+-------------+\n",
      "\n",
      "22/11/16 10:12:30 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 34172714 ms exceeds timeout 120000 ms\n",
      "22/11/16 10:12:31 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "# Directly aggregate funcion in column\n",
    "df_pyspark.agg({'msPlayed': 'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761094cf",
   "metadata": {},
   "source": [
    "# Part 6\n",
    "- ml with Dataframes\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Predict values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c08eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SixthSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cc7a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n",
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "training = spark.read.csv('../Pyspark-With-Python-main/test1.csv', header=True, inferSchema=True)\n",
    "training.show()\n",
    "training.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a group with independent features\n",
    "# Creating a vector assembler - will make sure thatall my features together grouped\n",
    "[Age,Experience]----> new feature--->independent feature # -> features: Age, Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84da8feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(inputCols=[\"age\", \"Experience\"], outputCol=\"Independent Features\")\n",
    "# inputCols parameter: will taking to group columns -> Needded be a numerical representation \n",
    "# outputCol parameter: \n",
    "# ! Strings can be converted to numerical with pyspark.ml libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2820e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+--------------------+\n",
      "|     Name|age|Experience|Salary|Independent Features|\n",
      "+---------+---+----------+------+--------------------+\n",
      "|    Krish| 31|        10| 30000|         [31.0,10.0]|\n",
      "|Sudhanshu| 30|         8| 25000|          [30.0,8.0]|\n",
      "|    Sunny| 29|         4| 20000|          [29.0,4.0]|\n",
      "|     Paul| 24|         3| 20000|          [24.0,3.0]|\n",
      "|   Harsha| 21|         1| 15000|          [21.0,1.0]|\n",
      "|  Shubham| 23|         2| 18000|          [23.0,2.0]|\n",
      "+---------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output = featureassembler.transform(training)\n",
    "output.show()\n",
    "\n",
    "#Basicaly Combined two columns in one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9285c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent Features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Needded the features to training: Salary will be a output desired and \"Independent Features\" the input\n",
    "finalized_data = output.select(\"Independent Features\", \"Salary\")\n",
    "finalized_data.show()\n",
    "\n",
    "# Salary its dependency feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6171b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 13:25:50 WARN Instrumentation: [e96da753] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "# Train test split\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "splits = finalized_data.randomSplit([0.75, 0.25])\n",
    "train_data = splits[0]\n",
    "test_data = splits[1]\n",
    "\n",
    "# featuresCol: how many number of feature columns are present\n",
    "# labelCol: the second feature that be the output feature\n",
    "regressor = LinearRegression(featuresCol='Independent Features', labelCol='Salary')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d111e690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [2200.0000000008085,-1400.0000000010095]\n",
      "intercept:  -29800.000000016233\n"
     ]
    }
   ],
   "source": [
    "print(\"coefficients: \", regressor.coefficients)\n",
    "print(\"intercept: \", regressor.intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "16084671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent Features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [24.0,3.0]| 20000|18800.000000000146|\n",
      "|          [29.0,4.0]| 20000| 28400.00000000318|\n",
      "|         [31.0,10.0]| 30000|24399.999999998727|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results=regressor.evaluate(test_data)\n",
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0edb0544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5066.666666668102, 34453333.333355784)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9f35e3",
   "metadata": {},
   "source": [
    "# Part 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20986cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('SeventhSession').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2078491a",
   "metadata": {},
   "source": [
    "[spotify dictionary dataset](https://www.kaggle.com/datasets/maharshipandya/-spotify-tracks-dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfaa8f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genre: string (nullable = true)\n",
      " |-- artist_name: string (nullable = true)\n",
      " |-- track_name: string (nullable = true)\n",
      " |-- track_id: string (nullable = true)\n",
      " |-- popularity: string (nullable = true)\n",
      " |-- acousticness: string (nullable = true)\n",
      " |-- danceability: string (nullable = true)\n",
      " |-- duration_ms: string (nullable = true)\n",
      " |-- energy: string (nullable = true)\n",
      " |-- instrumentalness: string (nullable = true)\n",
      " |-- key: string (nullable = true)\n",
      " |-- liveness: string (nullable = true)\n",
      " |-- loudness: string (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      " |-- speechiness: string (nullable = true)\n",
      " |-- tempo: string (nullable = true)\n",
      " |-- time_signature: string (nullable = true)\n",
      " |-- valence: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 31:=============================>                            (2 + 2) / 4]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spotify = spark.read.csv('../asset/SpotifyFeatures.csv', header=True, inferSchema=True)\n",
    "df_spotify.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "652c187a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+----------+------+------------+--------+-------+-------+-----------+\n",
      "|      artist_name|          track_name|popularity|energy|danceability|liveness|valence|  tempo|duration_ms|\n",
      "+-----------------+--------------------+----------+------+------------+--------+-------+-------+-----------+\n",
      "|   Henri Salvador|C'est beau de fai...|         0|  0.91|       0.389|   0.346|  0.814|166.969|      99373|\n",
      "|Martin & les fées|Perdu d'avance (p...|         1| 0.737|        0.59|   0.151|  0.816|174.003|     137373|\n",
      "|  Joseph Williams|Don't Let Me Be L...|         3| 0.131|       0.663|   0.103|  0.368| 99.488|     170267|\n",
      "|   Henri Salvador|Dis-moi Monsieur ...|         0| 0.326|        0.24|  0.0985|  0.227|171.758|     152427|\n",
      "|     Fabien Nataf|           Ouverture|         4| 0.225|       0.331|   0.202|   0.39|140.576|      82625|\n",
      "+-----------------+--------------------+----------+------+------------+--------+-------+-------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_spotify = df.select(['artist_name','track_name','popularity','energy','danceability','liveness','valence','tempo','duration_ms'])\n",
    "df_spotify.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a02b1c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('../Pyspark-With-Python-main/tips.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75396328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string category features into numerical features\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a46f69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|\n",
      "+----------+----+------+------+---+------+----+-----------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|\n",
      "+----------+----+------+------+---+------+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCol='sex', outputCol='sex_indexed')\n",
    "df_r = indexer.fit(df).transform(df)\n",
    "df_r.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "80919dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_indexed|smoker_indexed|day_indexed|time_indexed|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|        1.0|           0.0|        1.0|         0.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|        0.0|           0.0|        1.0|         0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|        0.0|           0.0|        1.0|         0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|        1.0|           0.0|        1.0|         0.0|\n",
      "+----------+----+------+------+---+------+----+-----------+--------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "indexer = StringIndexer(inputCols=['smoker','day','time'], outputCols=['smoker_indexed','day_indexed','time_indexed'])\n",
    "df_r = indexer.fit(df_r).transform(df_r)\n",
    "df_r.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0aed83b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|Independent Features|\n",
      "+--------------------+\n",
      "|[1.01,2.0,1.0,0.0...|\n",
      "|[1.66,3.0,0.0,0.0...|\n",
      "|[3.5,3.0,0.0,0.0,...|\n",
      "|[3.31,2.0,0.0,0.0...|\n",
      "|[3.61,4.0,1.0,0.0...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler = VectorAssembler(\n",
    "    inputCols=['tip','size','sex_indexed','smoker_indexed','day_indexed','time_indexed'], \n",
    "    outputCol='Independent Features'\n",
    ")\n",
    "output = featureassembler.transform(df_r)\n",
    "output.select('Independent Features').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46d52dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|Independent Features|total_bill|\n",
      "+--------------------+----------+\n",
      "|[1.01,2.0,1.0,0.0...|     16.99|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34|\n",
      "|[3.5,3.0,0.0,0.0,...|     21.01|\n",
      "|[3.31,2.0,0.0,0.0...|     23.68|\n",
      "|[3.61,4.0,1.0,0.0...|     24.59|\n",
      "+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dependent feature: total_bill\n",
    "finalized_data = output.select(['Independent Features', 'total_bill'])\n",
    "finalized_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "95d78436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/16 20:17:17 WARN Instrumentation: [d3c52da3] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "splits = finalized_data.randomSplit([0.75,0.25])\n",
    "train_data = splits[0]\n",
    "test_data = splits[1]\n",
    "\n",
    "regressor = LinearRegression(featuresCol='Independent Features', labelCol='total_bill')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4f3a0fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "coefficients:  [2.8212709416108286,3.2744188449046723,-0.543140804589959,1.9517834155603933,-0.2829488601637782,-1.3024082933796186] \n",
      "intercept:  2.8540073861909496\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"coefficients: \", regressor.coefficients, \"\\n\"\n",
    "    \"intercept: \", regressor.intercept\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ea49d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|Independent Features|total_bill|        prediction|\n",
      "+--------------------+----------+------------------+\n",
      "|(6,[0,1],[1.47,2.0])|     10.77|13.550113360168211|\n",
      "|(6,[0,1],[1.97,2.0])|     12.02|14.960748830973628|\n",
      "| (6,[0,1],[2.0,2.0])|     12.69| 15.04538695922195|\n",
      "|(6,[0,1],[2.34,4.0])|     17.81| 22.55345676917898|\n",
      "|(6,[0,1],[2.64,3.0])|     17.59|20.125419206757556|\n",
      "|(6,[0,1],[2.72,2.0])|     13.28| 17.07670203718175|\n",
      "|(6,[0,1],[3.15,3.0])|     20.08|21.564267386979076|\n",
      "|(6,[0,1],[5.92,3.0])|     29.03| 29.37918789524107|\n",
      "|[1.0,1.0,1.0,0.0,...|      7.25|  8.40655636811649|\n",
      "|[1.0,2.0,0.0,1.0,...|      12.6|14.175899433171516|\n",
      "|[1.0,2.0,1.0,1.0,...|      5.75|12.783912048090222|\n",
      "|[1.48,2.0,0.0,0.0...|      8.52|11.710020055877145|\n",
      "|[1.5,2.0,0.0,0.0,...|     19.08| 11.76644547470936|\n",
      "|[1.5,2.0,0.0,0.0,...|     12.46|  12.7859049079252|\n",
      "|[1.5,2.0,0.0,1.0,...|     15.69|15.303586043813151|\n",
      "|[1.5,2.0,0.0,1.0,...|     12.03|14.737688323485596|\n",
      "|[1.5,2.0,1.0,0.0,...|     11.17|  11.2233046701194|\n",
      "|[1.57,2.0,0.0,0.0...|     15.42|13.549291594165517|\n",
      "|[1.73,2.0,0.0,0.0...|      9.78|12.415337791279853|\n",
      "|[1.92,1.0,0.0,1.0...|      8.58|11.345794980677852|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Predictions\n",
    "pred_results = regressor.evaluate(test_data)\n",
    "pred_results.predictions.show()\n",
    "\n",
    "# total_bill is actual value and prediction is prediction value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3efbab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6697109355118465, 3.8239946020649587, 29.462956306379812)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Peformance Metrics\n",
    "pred_results.r2,pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
